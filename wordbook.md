# 単語帳

|名前   |説明   |リンク |
|:--    |:--    |:--    |
|パーセプトロン|視覚と脳の機能をモデル化したものであり、パターン認識を行う。|[５分でわかる！パーセプトロンの仕組みと実装方法（Python）](https://blog.apar.jp/deep-learning/11979/)<br>[機械学習の元祖「パーセプトロン」とは？](https://rightcode.co.jp/blog/information-technology/simple-perceptron)|
|多層パーセプトロン|多層パーセプトロン（たそうパーセプトロン、英: Multilayer perceptron、略称: MLP）は、順伝播型（英語版）ニューラルネットワークの一分類である。MLPは少なくとも3つのノードの層からなる。入力ノードを除けば、個々のノードは非線形活性化関数を使用するニューロンである。|[多層パーセプトロンWiki](https://ja.wikipedia.org/wiki/%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3)<br>[多層パーセプトロン(MLP)とは](https://tmytokai.github.io/open-ed/activity/dlearning/text02/page02.html)|
|活性化関数（activation function）|ニューラルネットワークのニューロンにおける、入力のなんらかの合計（しばしば、線形な重み付け総和）から、出力を決定するための関数で、非線形な関数とすることが多い。|[活性化関数(activation function)[ディープラーニング向け]](https://cvml-expertguide.net/terms/dl/layers/activation-function/)<br>[活性化関数（Activation function）とは？](https://atmarkit.itmedia.co.jp/ait/articles/2003/26/news012.html)|
|ReLU(Rectified Linear Unit)関数|入力が0を超えていれば、その入力をそのまま出力し0以下ならば0を出力する関数|[ReLU関数(ランプ関数,正規化線形関数)とは](https://mathlandscape.com/relu/)<br>[ReLU とその発展型の活性化関数【GELU, Swish, Mish など】](https://cvml-expertguide.net/terms/dl/layers/activation-function/relu-like-activation/)|
|ソフトマックス関数（Softmax function）<br> 正規化指数関数（Normalized exponential function）|複数の出力値の合計が1.0（＝100％）になるように変換して出力する関数|[［活性化関数］ソフトマックス関数（Softmax function）とは？](https://atmarkit.itmedia.co.jp/ait/articles/2004/08/news016.html)|
|損失関数(loss function)|「正解値」と、モデルによる出力された「予測値」とのズレの大きさ（これを「Loss：損失」と呼ぶ）を計算するための関数|[損失関数（Loss function）とは？　誤差関数／コスト関数／目的関数との違い](https://atmarkit.itmedia.co.jp/ait/articles/2104/15/news030.html)<br>[損失関数とは？ニューラルネットワークの学習理論【機械学習】](https://rightcode.co.jp/blog/information-technology/loss-function-neural-network-learning-theory)|
|偏微分（partial differential）|多変数関数を「特定の文字以外定数とみなして」微分したもののこと|[偏微分の意味と計算例・応用](https://manabitimes.jp/math/876)<br>[［AI・機械学習の数学］偏微分の基本（意味と計算方法）を理解する](https://atmarkit.itmedia.co.jp/ait/articles/2007/14/news021.html)|
|確率的勾配降下法(stochastic gradient descent):SGD|無作為に選び出したデータに対して行う勾配降下法|[確率的勾配降下法 wiki](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95)|
|ハイパーパラメータ|「重み」や「バイアス」などのパラメータとは異なり、手動によって設定されるパラメータ|[ハイパーパラメータ Wiki](https://ja.wikipedia.org/wiki/%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF)<br>[ハイパーパラメータとは？チューニングの手法を徹底解説（XGBoost編）](https://www.codexa.net/hyperparameter-tuning-python/)|
|エポック数|学習において訓練データをすべて使い切った時の回数|[エポック(epoch)数とは【機械学習 / Deep Learning】](https://www.st-hakky-blog.com/entry/2017/01/17/165137)|