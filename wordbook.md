# 単語帳

|名前   |説明   |リンク |
|:--    |:--    |:--    |
|パーセプトロン|視覚と脳の機能をモデル化したものであり、パターン認識を行う。|[５分でわかる！パーセプトロンの仕組みと実装方法（Python）](https://blog.apar.jp/deep-learning/11979/)<br>[機械学習の元祖「パーセプトロン」とは？](https://rightcode.co.jp/blog/information-technology/simple-perceptron)|
|多層パーセプトロン|多層パーセプトロン（たそうパーセプトロン、英: Multilayer perceptron、略称: MLP）は、順伝播型（英語版）ニューラルネットワークの一分類である。MLPは少なくとも3つのノードの層からなる。入力ノードを除けば、個々のノードは非線形活性化関数を使用するニューロンである。|[多層パーセプトロンWiki](https://ja.wikipedia.org/wiki/%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3)<br>[多層パーセプトロン(MLP)とは](https://tmytokai.github.io/open-ed/activity/dlearning/text02/page02.html)|
|活性化関数（activation function）|ニューラルネットワークのニューロンにおける、入力のなんらかの合計（しばしば、線形な重み付け総和）から、出力を決定するための関数で、非線形な関数とすることが多い。|[活性化関数(activation function)[ディープラーニング向け]](https://cvml-expertguide.net/terms/dl/layers/activation-function/)<br>[活性化関数（Activation function）とは？](https://atmarkit.itmedia.co.jp/ait/articles/2003/26/news012.html)|
|ReLU(Rectified Linear Unit)関数|入力が0を超えていれば、その入力をそのまま出力し0以下ならば0を出力する関数|[ReLU関数(ランプ関数,正規化線形関数)とは](https://mathlandscape.com/relu/)<br>[ReLU とその発展型の活性化関数【GELU, Swish, Mish など】](https://cvml-expertguide.net/terms/dl/layers/activation-function/relu-like-activation/)|
|ソフトマックス関数（Softmax function）<br> 正規化指数関数（Normalized exponential function）|複数の出力値の合計が1.0（＝100％）になるように変換して出力する関数|[［活性化関数］ソフトマックス関数（Softmax function）とは？](https://atmarkit.itmedia.co.jp/ait/articles/2004/08/news016.html)|
|損失関数(loss function)|「正解値」と、モデルによる出力された「予測値」とのズレの大きさ（これを「Loss：損失」と呼ぶ）を計算するための関数|[損失関数（Loss function）とは？　誤差関数／コスト関数／目的関数との違い](https://atmarkit.itmedia.co.jp/ait/articles/2104/15/news030.html)<br>[損失関数とは？ニューラルネットワークの学習理論【機械学習】](https://rightcode.co.jp/blog/information-technology/loss-function-neural-network-learning-theory)|
|偏微分（partial differential）|多変数関数を「特定の文字以外定数とみなして」微分したもののこと|[偏微分の意味と計算例・応用](https://manabitimes.jp/math/876)<br>[［AI・機械学習の数学］偏微分の基本（意味と計算方法）を理解する](https://atmarkit.itmedia.co.jp/ait/articles/2007/14/news021.html)|
|確率的勾配降下法(stochastic gradient descent):SGD|無作為に選び出したデータに対して行う勾配降下法|[確率的勾配降下法 wiki](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95)|
|ハイパーパラメータ|「重み」や「バイアス」などのパラメータとは異なり、手動によって設定されるパラメータ|[ハイパーパラメータ Wiki](https://ja.wikipedia.org/wiki/%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF)<br>[ハイパーパラメータとは？チューニングの手法を徹底解説（XGBoost編）](https://www.codexa.net/hyperparameter-tuning-python/)|
|エポック数|学習において訓練データをすべて使い切った時の回数|[エポック(epoch)数とは【機械学習 / Deep Learning】](https://www.st-hakky-blog.com/entry/2017/01/17/165137)|
|連鎖律(chain rule)|合成関数の微分についての性質であり、次のように定義される。「ある関数が合成関数で表される場合、その合成関数の微分は合成関数を構成するそれぞれの関数の微分の積によって表すことができる。」|[連鎖律 Wiki](https://ja.wikipedia.org/wiki/%E9%80%A3%E9%8E%96%E5%BE%8B)<br>[合成関数の偏微分における連鎖律(チェインルール)とその証明](https://mathlandscape.com/partial-derivative-composite/)|
|学習係数の減衰(learning rate decay)|深層学習の汎化性能向上のためによく使われる手法で、学習がある程度進んだ場所で学習率を下げる手法|[学習率減衰/バッチサイズ増大とEarlyStoppingの併用で汎化性能を上げる@tensorflow2.0](https://akichan-f.medium.com/%E5%AD%A6%E7%BF%92%E7%8E%87%E6%B8%9B%E8%A1%B0-%E3%83%90%E3%83%83%E3%83%81%E3%82%B5%E3%82%A4%E3%82%BA%E5%A2%97%E5%A4%A7%E3%81%A8earlystopping%E3%81%AE%E4%BD%B5%E7%94%A8%E3%81%A7%E6%B1%8E%E5%8C%96%E6%80%A7%E8%83%BD%E3%82%92%E4%B8%8A%E3%81%92%E3%82%8B-tensorflow2-0-2d8f2e3709f4)|
|Xavier の初期値|各層のアクティベーションが同様の広がりの分布になるように、前層のノードがn個のとき、平均0,標準偏差1n√1nである正規分布から初期値を生成する|[Xavierの初期値とは？？　〜機械学習の用語まとめ〜](https://qiita.com/Becon147/items/a9971041bca5c10483bc)<br>[Xavierの初期値の元論文](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)|
|Batch Normalization(バッチ正規化)|インプットとなる特徴量だけを正規化するのではなく、レイヤごとにインプットを正規化する。またその際にミニバッチを単位として、ミニバッチごとに正規化を行う。|[Batch Normalizationを理解する](https://data-analytics.fun/2021/09/11/understanding-batch-normalization/)<br>[Batch Normalization原論文](https://arxiv.org/abs/1502.03167)|
