# 単語帳

|名前   |説明   |リンク |
|:--    |:--    |:--    |
|パーセプトロン|視覚と脳の機能をモデル化したものであり、パターン認識を行う。|[５分でわかる！パーセプトロンの仕組みと実装方法（Python）](https://blog.apar.jp/deep-learning/11979/)<br>[機械学習の元祖「パーセプトロン」とは？](https://rightcode.co.jp/blog/information-technology/simple-perceptron)|
|多層パーセプトロン|多層パーセプトロン（たそうパーセプトロン、英: Multilayer perceptron、略称: MLP）は、順伝播型（英語版）ニューラルネットワークの一分類である。MLPは少なくとも3つのノードの層からなる。入力ノードを除けば、個々のノードは非線形活性化関数を使用するニューロンである。|[多層パーセプトロンWiki](https://ja.wikipedia.org/wiki/%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3)<br>[多層パーセプトロン(MLP)とは](https://tmytokai.github.io/open-ed/activity/dlearning/text02/page02.html)|
|活性化関数（activation function）|ニューラルネットワークのニューロンにおける、入力のなんらかの合計（しばしば、線形な重み付け総和）から、出力を決定するための関数で、非線形な関数とすることが多い。|[活性化関数(activation function)[ディープラーニング向け]](https://cvml-expertguide.net/terms/dl/layers/activation-function/)<br>[活性化関数（Activation function）とは？](https://atmarkit.itmedia.co.jp/ait/articles/2003/26/news012.html)|
|ReLU(Rectified Linear Unit)関数|入力が0を超えていれば、その入力をそのまま出力し0以下ならば0を出力する関数|[ReLU関数(ランプ関数,正規化線形関数)とは](https://mathlandscape.com/relu/)<br>[ReLU とその発展型の活性化関数【GELU, Swish, Mish など】](https://cvml-expertguide.net/terms/dl/layers/activation-function/relu-like-activation/)|
|ソフトマックス関数（Softmax function）<br> 正規化指数関数（Normalized exponential function）|複数の出力値の合計が1.0（＝100％）になるように変換して出力する関数|[［活性化関数］ソフトマックス関数（Softmax function）とは？](https://atmarkit.itmedia.co.jp/ait/articles/2004/08/news016.html)|